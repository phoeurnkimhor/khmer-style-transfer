---
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
header-includes: \usepackage{graphicx}
---


\begin{titlepage}
\thispagestyle{empty} 
\centering

\begin{tabular}{p{0.25\textwidth} p{0.5\textwidth} p{0.25\textwidth}}
\includegraphics[width=0.15\textwidth]{itc.png} & 
\centering \makebox[0.5\textwidth]{\Large \textbf{Institute of Technology of Cambodia}} &
\includegraphics[width=0.2\textwidth]{ams_t.png} \\[0.2cm]
& \centering \makebox[0.5\textwidth]{\Large \textbf{Department of Applied Mathematics and Statistics}} &
\end{tabular}


\vspace{2cm}

{\Huge \textbf{LSTM-based Khmer Text Style Transfer Using Representation Learning}}\par

\begin{center}
{\Large Subject: Natural Language Processing (NLP)} \\
\end{center}

\begin{center} 
{\large
\begin{tabular}{ l c }
\textbf{Name of Students} & \textbf{ID} \\
\hline
PEL Bunkhloem & e20201314 \\
PHETH Soriyuon & e20210674 \\
PHOEUN Rajame & e20211748 \\
PHOEURN Kimhor & e20210823 \\
PHORN Sreypov & e20210166 \\
YIN Sambat & e20210138 \\
\end{tabular}
}
\end{center}


\vspace{1cm} 
\begin{flushleft} 
Lecturers: \textbf{Dr. KHON Vanny (Course)} \\
\hspace{1.45cm} \textbf{Mr. TOUCH Sopheak (TP)}
\end{flushleft} 

\vfill

\begin{center} 
\textbf{Academic Year 2025 - 2026 }
\end{center}

\end{titlepage}

# 1. Introduction

The Khmer language employs multiple sociolinguistic registers that vary according to the social status and role of the person being addressed. These registers include **រាជសព្ទ (Royal language)** used when addressing or referring to the King, **សង្ឃសព្ទ (Monk language)** used for Buddhist monks, and polite or respectful speech used when addressing elders. Each register is characterized by distinct vocabulary choices and linguistic conventions, making correct usage essential for cultural and social appropriateness.

The objective of this project is to develop an automated system capable of converting standard Khmer words or sentences into their corresponding sociolinguistic forms. This task can be viewed as a form of *style transfer* in natural language processing, where the semantic meaning of the input text is preserved while the linguistic style is transformed.

For example, a commonly used verb such as **"ញ៉ាំ"** (to eat) should be accurately transformed into:

- **"សោយ"** when referring to a King  
- **"ឆាន់"** when referring to a Monk  
- **"ពិសា"** when addressing or referring to an elder

Achieving such transformations requires the model to understand both contextual meaning and sociolinguistic rules embedded within the Khmer language.

To address this challenge, the project explores the use of deep learning–based sequence models, particularly **Recurrent Neural Networks (RNNs)**, which are well-suited for learning sequential patterns in text. By training the model on parallel datasets consisting of standard Khmer text and its corresponding formalized versions, the system aims to learn appropriate mappings between different linguistic registers.

This project contributes to Khmer language processing by supporting automated formalization, promoting correct language usage in digital applications, and addressing the challenges of style transfer in a low-resource language setting.





# 2. Problem Statement

## 2.1 Definition of the Problem

The Khmer language contains multiple sociolinguistic registers that depend on the social status of the subject being referenced. Among these, **Royal Khmer (រាជសព្ទ)** represents the most formal register and differs significantly from normal Khmer text in terms of vocabulary usage and stylistic expression.

The objective of this project is to develop an automated text transformation system that converts **normal Khmer text into Royal Khmer text**. Due to limited time and resources, the scope of this study is restricted to a **single transformation direction**, namely from normal text to royal text. Other transformations, such as monk language or elder-respectful language, are not considered.

A major challenge of this problem is the **lack of publicly available labeled datasets** for Khmer sociolinguistic style transfer. Consequently, all training data must be **manually labeled**, which significantly limits the size of the dataset. Initial experimental results indicate that the model is not yet able to consistently produce correct royal-style text, often generating partially correct or stylistically inaccurate outputs. These observations highlight the difficulty of the task under low-resource conditions.

---

## 2.2 Mathematical Formulation

The task is formulated as a **sequence-to-sequence style transfer problem**, where an input sentence in normal Khmer is transformed into its corresponding royal-style sentence while preserving semantic content.

Let:

- \( X = (x_1, x_2, \dots, x_T) \) denote the input sequence in normal Khmer  
- \( Y = (y_1, y_2, \dots, y_{T'}) \) denote the output sequence in Royal Khmer  
- \( f_\theta \) denote the encoder–decoder LSTM model parameterized by \( \theta \)

The conditional probability of generating the target sequence given the input is modeled as:

\[
P(Y \mid X; \theta) = \prod_{t=1}^{T'} P(y_t \mid y_1, \dots, y_{t-1}, X; \theta)
\]

The optimal parameters \( \theta^* \) are obtained by maximizing the log-likelihood over the labeled dataset \( \mathcal{D} \):

\[
\theta^* = \arg\max_\theta \sum_{(X, Y) \in \mathcal{D}} \log P(Y \mid X; \theta)
\]

This formulation treats Khmer style transfer as a **representation learning problem**, where the encoder learns latent representations of the input sentence that can be decoded into the royal linguistic register.

---

## 2.3 Explanation of Terms and Notation

- \( X \): Input sequence of tokens representing normal Khmer text  
- \( Y \): Target sequence of tokens representing Royal Khmer text  
- \( x_t \): The \(t\)-th token in the input sequence  
- \( y_t \): The \(t\)-th token in the output sequence  
- \( T \), \( T' \): Lengths of the input and output sequences  
- \( \mathcal{D} \): Manually labeled dataset of paired normal–royal Khmer sentences  
- \( \theta \): Trainable parameters of the encoder–decoder LSTM model  
- \( f_\theta \): Sequence-to-sequence function learned by the model  
- \( h_t \): Encoder hidden state representing contextual information at time step \(t\)  

---

## Remarks on Current Model Performance

Based on early experimental results, the model does not yet reliably perform correct style conversion. While some semantic content is preserved, royal-specific vocabulary and stylistic markers are frequently missing or incorrectly generated. These limitations are primarily attributed to the small size of the manually labeled dataset, limited training time, and the inherent complexity of Khmer royal language conventions.


# 2. Proposed Solution

We use an **encoder-decoder LSTM** for Khmer style transfer, adopting a **pretraining + fine-tuning approach** to improve performance on limited paired data.

## Pretraining

Before fine-tuning, we **pretrained a single LSTM** on a large corpus of general Khmer text. The pretraining task was **self-reconstruction** \((X \to X)\), meaning that given a sequence \(X = (x_1, x_2, \dots, x_T)\), the model attempts to rewrite it.  

- Pretraining loss:

\[
\mathcal{L}_{pretrain}(\theta) = - \sum_{X \in \mathcal{C}} \sum_{t=1}^{T} \log P(x_t \mid x_1, \dots, x_{t-1}; \theta)
\]

- Here, \(\theta\) includes all LSTM weights and biases.  
- Purpose: learn **robust Khmer token embeddings and sequence representations**.  

The pretrained LSTM weights are then **used to initialize the encoder and/or decoder** of the style-transfer encoder-decoder model, giving the model a good starting point.


## Model Architecture

### Encoder LSTM

The encoder reads the input normal-style sentence \(X\) and generates **hidden states** \(h_t\) and **cell states** \(c_t\):

\[
\begin{aligned}
f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) \\
i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) \\
o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o) \\
\tilde{c}_t &= \tanh(W_c x_t + U_c h_{t-1} + b_c) \\
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\
h_t &= o_t \odot \tanh(c_t)
\end{aligned}
\]

- \(c_t\) stores long-term information.  
- \(h_t\) represents the content of the input sentence.  
- Encoder weights can be initialized with pretrained LSTM weights.

### Decoder LSTM

The decoder generates the royal-style target sequence \(Y = (y_1, \dots, y_{T'})\) token by token:

\[
P(y_t \mid y_{<t}, X; \theta) = \text{Softmax}(W s_t + b)
\]

- \(s_t\) = decoder hidden state at step \(t\)  
- Each token depends on previous outputs and encoder representations.  
- Decoder weights can also be initialized with pretrained LSTM weights.


## Fine-Tuning for Style Transfer

The fine-tuning loss is **negative log-likelihood** over the paired dataset \(\mathcal{D}\):

\[
\mathcal{L}_{finetune}(\theta) = - \sum_{(X, Y) \in \mathcal{D}} \sum_{t=1}^{T'} \log P(y_t \mid y_1, \dots, y_{t-1}, X; \theta)
\]

- Fine-tuning adapts pretrained weights \(\theta\) to the **style transfer task**.  
- The model learns to **rewrite normal Khmer sentences into royal style**, while preserving content.


