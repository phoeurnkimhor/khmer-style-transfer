% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{graphicx}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{\vspace{-2.5em}}

\begin{document}

\begin{titlepage}
\thispagestyle{empty} 
\centering

\begin{tabular}{p{0.25\textwidth} p{0.5\textwidth} p{0.25\textwidth}}
\includegraphics[width=0.15\textwidth]{itc.png} & 
\centering \makebox[0.5\textwidth]{\Large \textbf{Institute of Technology of Cambodia}} &
\includegraphics[width=0.2\textwidth]{ams_t.png} \\[0.2cm]
& \centering \makebox[0.5\textwidth]{\Large \textbf{Department of Applied Mathematics and Statistics}} &
\end{tabular}


\vspace{2cm}

{\Huge \textbf{LSTM-based Khmer Text Style Transfer Using Representation Learning}}\par

\begin{center}
{\Large Subject: Natural Language Processing (NLP)} \\
\end{center}

\begin{center} 
{\large
\begin{tabular}{ l c }
\textbf{Name of Students} & \textbf{ID} \\
\hline
PEL Bunkhloem & e20201314 \\
PHETH Soriyuon & e20210674 \\
PHOEUN Rajame & e20211748 \\
PHOEURN Kimhor & e20210823 \\
PHORN Sreypov & e20210166 \\
YIN Sambat & e20210138 \\
\end{tabular}
}
\end{center}


\vspace{1cm} 
\begin{flushleft} 
Lecturers: \textbf{Dr. KHON Vanny (Course)} \\
\hspace{1.45cm} \textbf{Mr. TOUCH Sopheak (TP)}
\end{flushleft} 

\vfill

\begin{center} 
\textbf{Academic Year 2025 - 2026 }
\end{center}

\end{titlepage}

\section{1. Problem Statement}\label{problem-statement}

The task is to transfer text from a \textbf{source style} to a
\textbf{target style} while preserving content. Formally, let:

\begin{itemize}
\tightlist
\item
  \(X = (x_1, x_2, \dots, x_T)\) denote the input sequence in the source
  style.\\
\item
  \(Y = (y_1, y_2, \dots, y_{T'})\) denote the target sequence in the
  desired style.\\
\item
  \(f_\theta\) be the function implemented by our seq2seq LSTM,
  parameterized by \(\theta\).
\end{itemize}

We aim to model the conditional distribution:

\[
P(Y \mid X; \theta) = \prod_{t=1}^{T'} P(y_t \mid y_1, \dots, y_{t-1}, X; \theta)
\]

The objective is to find \(\theta^*\) that maximizes the likelihood of
the target sequences in the dataset \(\mathcal{D}\):

\[
\theta^* = \arg\max_\theta \sum_{(X, Y) \in \mathcal{D}} \log P(Y \mid X; \theta)
\]

This is a \textbf{representation learning problem}, as the encoder LSTM
learns hidden states \(h_t\) that represent the content of the input
sequence in a way that can be decoded into the target style.

\section{2. Proposed Solution}\label{proposed-solution}

We use an \textbf{encoder-decoder LSTM} for Khmer style transfer,
adopting a \textbf{pretraining + fine-tuning approach} to improve
performance on limited paired data.

\subsection{Pretraining}\label{pretraining}

Before fine-tuning, we \textbf{pretrained a single LSTM} on a large
corpus of general Khmer text. The pretraining task was
\textbf{self-reconstruction} \((X \to X)\), meaning that given a
sequence \(X = (x_1, x_2, \dots, x_T)\), the model attempts to rewrite
it.

\begin{itemize}
\tightlist
\item
  Pretraining loss:
\end{itemize}

\[
\mathcal{L}_{pretrain}(\theta) = - \sum_{X \in \mathcal{C}} \sum_{t=1}^{T} \log P(x_t \mid x_1, \dots, x_{t-1}; \theta)
\]

\begin{itemize}
\tightlist
\item
  Here, \(\theta\) includes all LSTM weights and biases.\\
\item
  Purpose: learn \textbf{robust Khmer token embeddings and sequence
  representations}.
\end{itemize}

The pretrained LSTM weights are then \textbf{used to initialize the
encoder and/or decoder} of the style-transfer encoder-decoder model,
giving the model a good starting point.

\subsection{Model Architecture}\label{model-architecture}

\subsubsection{Encoder LSTM}\label{encoder-lstm}

The encoder reads the input normal-style sentence \(X\) and generates
\textbf{hidden states} \(h_t\) and \textbf{cell states} \(c_t\):

\[
\begin{aligned}
f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) \\
i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) \\
o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o) \\
\tilde{c}_t &= \tanh(W_c x_t + U_c h_{t-1} + b_c) \\
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\
h_t &= o_t \odot \tanh(c_t)
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  \(c_t\) stores long-term information.\\
\item
  \(h_t\) represents the content of the input sentence.\\
\item
  Encoder weights can be initialized with pretrained LSTM weights.
\end{itemize}

\subsubsection{Decoder LSTM}\label{decoder-lstm}

The decoder generates the royal-style target sequence
\(Y = (y_1, \dots, y_{T'})\) token by token:

\[
P(y_t \mid y_{<t}, X; \theta) = \text{Softmax}(W s_t + b)
\]

\begin{itemize}
\tightlist
\item
  \(s_t\) = decoder hidden state at step \(t\)\\
\item
  Each token depends on previous outputs and encoder representations.\\
\item
  Decoder weights can also be initialized with pretrained LSTM weights.
\end{itemize}

\subsection{Fine-Tuning for Style
Transfer}\label{fine-tuning-for-style-transfer}

The fine-tuning loss is \textbf{negative log-likelihood} over the paired
dataset \(\mathcal{D}\):

\[
\mathcal{L}_{finetune}(\theta) = - \sum_{(X, Y) \in \mathcal{D}} \sum_{t=1}^{T'} \log P(y_t \mid y_1, \dots, y_{t-1}, X; \theta)
\]

\begin{itemize}
\tightlist
\item
  Fine-tuning adapts pretrained weights \(\theta\) to the \textbf{style
  transfer task}.\\
\item
  The model learns to \textbf{rewrite normal Khmer sentences into royal
  style}, while preserving content.
\end{itemize}

\end{document}
