---
title: "LSTM-based Khmer Text Style Transfer Using Representation Learning"
author: 
  - "Pel Bunkhloem Phoeurn Kimhor"
  - "Pheth Soriyuon Phorn Sreypov"
  - "Phoeun Rajame Yin Sambat"
  - "**Supervised by:** Dr. KHON Vanny"
date: "02 January 2026"
format: 
  revealjs:
    slide-number: true
    transition: slide
    scrollable: true
    theme: white
    progress: true
    smaller: true
    embed-resources: true
    number-sections: true

include-in-header:
  - text: |
      <style>
      .title {
        font-size: 1.5em !important; /* Adjust this number to make it smaller */
      }
      .reveal .slide-background.present::before {
        content: '';
        position: absolute;
        top: 10px;
        left: 10px;
        width: 120px;
        height: 120px;
        background-image: url('../assets/itc.png');
        background-size: contain;
        background-repeat: no-repeat;
        z-index: 10;
      }
      .reveal .slide-background.present::after {
        content: '';
        position: absolute;
        top: 10px;
        right: 10px;
        width: 200px;
        height: 200px;
        background-image: url('../assets/ams_t.png');
        background-size: contain;
        background-repeat: no-repeat;
        z-index: 10;
      }
      .reveal .slides > section.present {
        position: relative;
      }
      .title-logos {
        position: absolute;
        top: 10px;
        width: 100%;
        display: flex;
        justify-content: space-between;
        padding: 0 20px;
        box-sizing: border-box;
        z-index: 20;
        pointer-events: none;
      }
      .title-logos img {
        height: 100px;
        width: auto;
      }
      .reveal h1 {
        font-size: 1.5em !important;
      }
      .reveal h2,
      .reveal h3 {
        font-size: 0.7em;
      }
      </style>
---

# Problem Statement

The task is formulated as a **sequence-to-sequence style transfer problem**, where an input sentence in normal Khmer is transformed into its corresponding royal-style sentence.

Let:

* $X = (x_1, x_2, \dots, x_T)$ denote the input sequence
* $Y = (y_1, y_2, \dots, y_{T'})$ denote the output sequence
* $f_\theta$ denote an encoder–decoder LSTM model

---

## Model Objective and Notation

<div style="display: flex; gap: 2rem; align-items: flex-start; font-size: 0.85em;">

<div style="flex: 0.65;">

<strong style="color: #1f77b4;">Conditional Probability</strong>


$$
P(Y \mid X; \theta)
=
\prod_{t=1}^{T'}
P\!\left(y_t \mid y_1, \dots, y_{t-1}, X; \theta\right)
$$

<strong style="color: #1f77b4;">Training Objective</strong>

$$
\theta^*
=
\arg\max_{\theta}
\sum_{(X,Y)\in\mathcal{D}}
\log P(Y \mid X; \theta)
$$

</div>

<div style="flex: 0.35; font-size: 0.9em;">

<strong style="color: #1f77b4;">Notation</strong>

- $x_t, y_t$ : tokens at time step $t$  
- $T, T'$ : input and output lengths  
- $\theta$ : model parameters  
- $h_t$ : encoder hidden state  

</div>

</div>

---

# Proposed Solution
## Pre-training

### Data Description
Data is scraped from:

- Sources: Diverse online Khmer articles (news, blogs, social media, wikipedia).
- Technique: Selenium.

---

### Data Preparation & Preprocessing

::: {.columns}

::: {.column width="50%"}
**Dataset Splitting**

* **Total:** Large Khmer corpus.
* **Filter:** Min length  chars.
* **Split:** 80% Train | 10% Val | 10% Test.
:::

::: {.column width="50%"}
**Cleaning Pipeline**

1. **Remove Noise:** special punctuation (។, ៗ).
2. **Script Focus:** Strip Latin chars/digits.
3. **Normalization:** Map redundant glyphs (e.g., ឝ  គ).
4. **Trim:** Remove extra whitespace.
:::

:::

---

### Tokenization 

* **Method:** Character-to-index mapping (`stoi`).
* **Special Tokens:** `<sos>`, `<eos>`, `<pad>`, `<unk>`.
* **Batching:** Sequences aligned via `pad_sequence`.

---

# Fine-tuning

## Dataset: Normal to Royal Transfer

::: {.columns}
::: {.column width="50%"}
**Parallel Corpus**

* **Source:** Scraped from various articles.
* **Volume:** 793 sentence pairs.
* **Format:**
    * **Input:** Normal Khmer (សាមញ្ញ)
    * **Target:** Royal Khmer (រាជស័ព្ទ)
:::

::: {.column width="50%"}
**Data Structure Example**

| Normal | Royal |
|-------|----------------|
| លោកបានដើរកាត់តាមឆ្នេរសមុទ្រ | ព្រះអង្គស្តេចយាងកាត់តាមឆ្នេរសមុទ្រ |

:::
:::

---

## Preprocessing & Alignment

::: {.columns}
::: {.column width="50%"}
**Text Cleaning**

- **Punctuation:** Removed traditional markers (៙, ៚, ៖, ។).
- **Normalization:** Whitespace trimming
:::

::: {.column width="50%"}
**Tokenization**

- **Level:** Character-level mapping (`stoi`).
- **Wrapping:** Added `<sos>` and `<eos>` to every sequence.
- **Splitting Strategy:**
    - **Train:** 80%
    - **Validation:** 10%
    - **Test:** 10%
:::
:::



## Attention Mechanism

The mechanism begins by calculating how well each encoder state $h_s^{enc}$ matches the current decoder needs.

**Alignment Score:** Measures the relevance of input $s$ at decoding step $t$:
$$e_{t,s} = h_{t-1}^{dec} \cdot h_s^{enc}$$

**Attention Weight:** Normalizes scores into a probability distribution using Softmax:
$$\alpha_{t,s} = \frac{\exp(e_{t,s})}{\sum_{k=1}^{T} \exp(e_{t,k})}$$

The model then extracts relevant information to generate the final character.

**Context Vector ($c_t$):** A weighted sum of all encoder hidden states:
$$c_t = \sum_{s=1}^{T} \alpha_{t,s} h_s^{enc}$$

**Final Prediction:** The decoder hidden state $h_t^{dec}$ is updated with $c_t$, and the next character is predicted:
$$P(y_t \mid y_{<t}, X) = \text{Softmax}(W_{hy} h_t^{dec} + b_y)$$

# Results

**Pre-training BLEU (test):**

| Model        | BLEU (%) |
|--------------|----------|
| General Text | **30.1** |
| Folktale Text| **9.4**  |

**Fine-tuning (samples):** BLEU scores range **0.73–0.87** across representative sentences, with outputs reflecting royal style and coherent structure.

| Generated Output | Actual Output | BLEU |
|-------|-----------------|------------|
| ព្រះអង្គទ្រង់បានសម្លាប់មន្ត្រីក្បត់ហើយ | ព្រះអង្គទ្រង់រៀបចំរាជាភិសេកថ្វាយព្រះរាជ | 0.87 |
| ព្រះនាងទ្រង់ប្រសូតនៅថ្ងៃទី ១ នី ២ នាក់ | ព្រះនាងថ្វាយព្រះសន្យាថានឹងថែរក្សាព្រះកិ | 0.79 |


