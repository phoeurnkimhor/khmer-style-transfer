{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14301143,"sourceType":"datasetVersion","datasetId":9129195},{"sourceId":699965,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":531078,"modelId":544937}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Load and Preprocess Data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-12-27T07:12:52.629270Z","iopub.execute_input":"2025-12-27T07:12:52.629648Z","iopub.status.idle":"2025-12-27T07:12:52.644146Z","shell.execute_reply.started":"2025-12-27T07:12:52.629620Z","shell.execute_reply":"2025-12-27T07:12:52.643576Z"}}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/kh-tst/normal-royal.csv')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:19:00.572514Z","iopub.execute_input":"2025-12-27T07:19:00.573237Z","iopub.status.idle":"2025-12-27T07:19:00.593688Z","shell.execute_reply.started":"2025-12-27T07:19:00.573179Z","shell.execute_reply":"2025-12-27T07:19:00.593083Z"}},"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"                                           normal  \\\n0                    លោកបានដើរកាត់តាមឆ្នេរសមុទ្រ។   \n1                              លោកបានទៅដល់កោះមួយ។   \n2                មានដើមធ្លកដុះបែកមែកត្រសុំត្រសាយ។   \n3               នៅក្រោមដើមឈើនោះមានរូងមួយយ៉ាងជ្រៅ។   \n4                    វាជាផ្លូវចេញចូលរបស់ស្ដេចនាគ។   \n..                                            ...   \n800           ស្ដេចគឺជាក្សត្រដែលពូកែធ្វើសង្គ្រាម។   \n801       ស្ដេចឱ្យគេឆ្លាក់រូបសត្វហង្សលើដំបូលវាំង។   \n802             ស្ដេចចូលចិត្តដើរលេងក្នុងព្រៃភ្នំ។   \n803  ស្ដេចឱ្យគេធ្វើឆត្រធំៗសម្រាប់ការពារកម្ដៅថ្ងៃ។   \n804         ស្ដេចមានចិត្តចង់ជួយខ្មែរឱ្យរស់បានយូរ។   \n\n                                                 royal  \n0                   ព្រះអង្គនិមន្ដកាត់តាមឆ្នេរមហាសាគរ។  \n1                                   បានយាងទៅដល់កោះមួយ។  \n2                 មានដើមធ្លកដុះបែកមែកសាខាត្រសុំត្រសាយ។  \n3                 នៅក្រោមដើមធ្លកនោះ មានរូងមួយយ៉ាងជ្រៅ។  \n4                  ដែលជាផ្លូវចេញចូលរបស់ស្ដេចភុជង្គនាគ។  \n..                                                 ...  \n800  ព្រះអង្គជាមហាក្សត្រដ៏ស្ទាត់ជំនាញក្នុងយុទ្ធសាស្...  \n801             ព្រះអង្គឱ្យរចនារូបហង្សលើព្រះរាជដំណាក់។  \n802     ព្រះអង្គសព្វព្រះរាជហឫទ័យយាងប្រពាសព្រៃព្រឹក្សា។  \n803       ព្រះអង្គឱ្យសាងសេតច្ឆត្រការពារព្រះកាយពីកម្ដៅ។  \n804   ព្រះអង្គមានព្រះរាជបំណងឱ្យខេមរជាតិស្ថិតស្ថេរអមតៈ។  \n\n[805 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>normal</th>\n      <th>royal</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>លោកបានដើរកាត់តាមឆ្នេរសមុទ្រ។</td>\n      <td>ព្រះអង្គនិមន្ដកាត់តាមឆ្នេរមហាសាគរ។</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>លោកបានទៅដល់កោះមួយ។</td>\n      <td>បានយាងទៅដល់កោះមួយ។</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>មានដើមធ្លកដុះបែកមែកត្រសុំត្រសាយ។</td>\n      <td>មានដើមធ្លកដុះបែកមែកសាខាត្រសុំត្រសាយ។</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>នៅក្រោមដើមឈើនោះមានរូងមួយយ៉ាងជ្រៅ។</td>\n      <td>នៅក្រោមដើមធ្លកនោះ មានរូងមួយយ៉ាងជ្រៅ។</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>វាជាផ្លូវចេញចូលរបស់ស្ដេចនាគ។</td>\n      <td>ដែលជាផ្លូវចេញចូលរបស់ស្ដេចភុជង្គនាគ។</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>800</th>\n      <td>ស្ដេចគឺជាក្សត្រដែលពូកែធ្វើសង្គ្រាម។</td>\n      <td>ព្រះអង្គជាមហាក្សត្រដ៏ស្ទាត់ជំនាញក្នុងយុទ្ធសាស្...</td>\n    </tr>\n    <tr>\n      <th>801</th>\n      <td>ស្ដេចឱ្យគេឆ្លាក់រូបសត្វហង្សលើដំបូលវាំង។</td>\n      <td>ព្រះអង្គឱ្យរចនារូបហង្សលើព្រះរាជដំណាក់។</td>\n    </tr>\n    <tr>\n      <th>802</th>\n      <td>ស្ដេចចូលចិត្តដើរលេងក្នុងព្រៃភ្នំ។</td>\n      <td>ព្រះអង្គសព្វព្រះរាជហឫទ័យយាងប្រពាសព្រៃព្រឹក្សា។</td>\n    </tr>\n    <tr>\n      <th>803</th>\n      <td>ស្ដេចឱ្យគេធ្វើឆត្រធំៗសម្រាប់ការពារកម្ដៅថ្ងៃ។</td>\n      <td>ព្រះអង្គឱ្យសាងសេតច្ឆត្រការពារព្រះកាយពីកម្ដៅ។</td>\n    </tr>\n    <tr>\n      <th>804</th>\n      <td>ស្ដេចមានចិត្តចង់ជួយខ្មែរឱ្យរស់បានយូរ។</td>\n      <td>ព្រះអង្គមានព្រះរាជបំណងឱ្យខេមរជាតិស្ថិតស្ថេរអមតៈ។</td>\n    </tr>\n  </tbody>\n</table>\n<p>805 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":85},{"cell_type":"code","source":"import re\nimport string\n\nunwanted_chars = ['\\u200b','\\u200c','\\u200d','\\ufeff','៙','៚','៖','ៗ','៛','។']\n\nkhmer_punct = ''\n\ndef clean_text(text):\n    text = ''.join(c for c in text if c not in unwanted_chars)\n    text = re.sub(r'[A-Za-z0-9]+', '', text)\n    allowed_chars = string.ascii_letters + string.digits\n    text = ''.join(c for c in text if c not in string.punctuation)\n    text = re.sub(r'[^\\u1780-\\u17FF\\u17E0-\\u17E9\\s' + khmer_punct + ']', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\ndf['normal'] = df['normal'].apply(clean_text)\ndf['royal'] = df['royal'].apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:19:00.595028Z","iopub.execute_input":"2025-12-27T07:19:00.595345Z","iopub.status.idle":"2025-12-27T07:19:00.634705Z","shell.execute_reply.started":"2025-12-27T07:19:00.595321Z","shell.execute_reply":"2025-12-27T07:19:00.634016Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"vocab = [' ', 'ក', 'ខ', 'គ', 'ឃ', 'ង', 'ច', 'ឆ', 'ជ', 'ឈ', 'ញ', 'ដ', 'ឋ', 'ឌ', 'ឍ', 'ណ', 'ត', 'ថ', 'ទ', 'ធ', 'ន', 'ប', 'ផ', 'ព', 'ភ', 'ម', 'យ', 'រ', 'ល', 'វ', 'ឝ', 'ឞ', 'ស', 'ហ', 'ឡ', 'អ', 'ឣ', 'ឤ', 'ឥ', 'ឦ', 'ឧ', 'ឩ', 'ឪ', 'ឫ', 'ឬ', 'ឭ', 'ឮ', 'ឯ', 'ឰ', 'ឱ', 'ឲ', 'ឳ', 'ា', 'ិ', 'ី', 'ឹ', 'ឺ', 'ុ', 'ូ', 'ួ', 'ើ', 'ឿ', 'ៀ', 'េ', 'ែ', 'ៃ', 'ោ', 'ៅ', 'ំ', 'ះ', 'ៈ', '៉', '៊', '់', '៌', '៍', '៏', '័', '៑', '្', '៝', '០', '១', '២', '៣', '៤', '៥', '៦', '៧', '៨', '៩']\nspecial_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n\n# Merge vocab\nfull_vocab = vocab + [tok for tok in special_tokens if tok not in vocab]\nvocab = {ch: i for i, ch in enumerate(full_vocab)}\nitos = {i: ch for ch, i in vocab.items()}\nvocab_size = len(vocab)\nprint(f\"Vocab size: {vocab_size}\\n\")\nprint(vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:19:00.635424Z","iopub.execute_input":"2025-12-27T07:19:00.635621Z","iopub.status.idle":"2025-12-27T07:19:00.642529Z","shell.execute_reply.started":"2025-12-27T07:19:00.635601Z","shell.execute_reply":"2025-12-27T07:19:00.641887Z"}},"outputs":[{"name":"stdout","text":"Vocab size: 95\n\n{' ': 0, 'ក': 1, 'ខ': 2, 'គ': 3, 'ឃ': 4, 'ង': 5, 'ច': 6, 'ឆ': 7, 'ជ': 8, 'ឈ': 9, 'ញ': 10, 'ដ': 11, 'ឋ': 12, 'ឌ': 13, 'ឍ': 14, 'ណ': 15, 'ត': 16, 'ថ': 17, 'ទ': 18, 'ធ': 19, 'ន': 20, 'ប': 21, 'ផ': 22, 'ព': 23, 'ភ': 24, 'ម': 25, 'យ': 26, 'រ': 27, 'ល': 28, 'វ': 29, 'ឝ': 30, 'ឞ': 31, 'ស': 32, 'ហ': 33, 'ឡ': 34, 'អ': 35, 'ឣ': 36, 'ឤ': 37, 'ឥ': 38, 'ឦ': 39, 'ឧ': 40, 'ឩ': 41, 'ឪ': 42, 'ឫ': 43, 'ឬ': 44, 'ឭ': 45, 'ឮ': 46, 'ឯ': 47, 'ឰ': 48, 'ឱ': 49, 'ឲ': 50, 'ឳ': 51, 'ា': 52, 'ិ': 53, 'ី': 54, 'ឹ': 55, 'ឺ': 56, 'ុ': 57, 'ូ': 58, 'ួ': 59, 'ើ': 60, 'ឿ': 61, 'ៀ': 62, 'េ': 63, 'ែ': 64, 'ៃ': 65, 'ោ': 66, 'ៅ': 67, 'ំ': 68, 'ះ': 69, 'ៈ': 70, '៉': 71, '៊': 72, '់': 73, '៌': 74, '៍': 75, '៏': 76, '័': 77, '៑': 78, '្': 79, '៝': 80, '០': 81, '១': 82, '២': 83, '៣': 84, '៤': 85, '៥': 86, '៦': 87, '៧': 88, '៨': 89, '៩': 90, '<pad>': 91, '<unk>': 92, '<sos>': 93, '<eos>': 94}\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"def sentence_to_char_indices(sentence, vocab):\n    chars = list(sentence)\n    return [vocab[\"<sos>\"]] + [vocab.get(c, vocab[\"<unk>\"]) for c in chars] + [vocab[\"<eos>\"]]\n\ndf['input_ids'] = df['normal'].apply(lambda s: sentence_to_char_indices(s, vocab))\ndf['target_ids'] = df['royal'].apply(lambda s: sentence_to_char_indices(s, vocab))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:19:00.643800Z","iopub.execute_input":"2025-12-27T07:19:00.644086Z","iopub.status.idle":"2025-12-27T07:19:00.669189Z","shell.execute_reply.started":"2025-12-27T07:19:00.644056Z","shell.execute_reply":"2025-12-27T07:19:00.668702Z"}},"outputs":[],"execution_count":88},{"cell_type":"markdown","source":"# 2. Start Training","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:19:00.669993Z","iopub.execute_input":"2025-12-27T07:19:00.670266Z","iopub.status.idle":"2025-12-27T07:19:00.689446Z","shell.execute_reply.started":"2025-12-27T07:19:00.670242Z","shell.execute_reply":"2025-12-27T07:19:00.688669Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass Seq2SeqDataset(Dataset):\n    def __init__(self, input_seqs, target_seqs):\n        self.input_seqs = input_seqs\n        self.target_seqs = target_seqs\n\n    def __len__(self):\n        return len(self.input_seqs)\n\n    def __getitem__(self, idx):\n        return self.input_seqs[idx], self.target_seqs[idx]\n\n# Collate function to pad sequences\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence([torch.tensor(seq) for seq in inputs], batch_first=True, padding_value=vocab['<pad>'])\n    targets = pad_sequence([torch.tensor(seq) for seq in targets], batch_first=True, padding_value=vocab['<pad>'])\n    return inputs, targets\n\ntrain_dataset = Seq2SeqDataset(train_df['input_ids'].reset_index(drop=True), train_df['target_ids'].reset_index(drop=True))\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n\nval_dataset = Seq2SeqDataset(val_df['input_ids'].reset_index(drop=True), val_df['target_ids'].reset_index(drop=True))\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n\ntest_dataset = Seq2SeqDataset(test_df['input_ids'].reset_index(drop=True), test_df['target_ids'].reset_index(drop=True))\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:48:11.916690Z","iopub.execute_input":"2025-12-27T07:48:11.916991Z","iopub.status.idle":"2025-12-27T07:48:11.926029Z","shell.execute_reply.started":"2025-12-27T07:48:11.916962Z","shell.execute_reply":"2025-12-27T07:48:11.925475Z"}},"outputs":[],"execution_count":118},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embed_size, hidden_size, num_layers=2):\n        super().__init__()\n        self.embedding = nn.Embedding(input_size, embed_size)\n        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True)\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        # x: [batch, seq_len]\n        embedded = self.embedding(x)  # [batch, seq_len, embed]\n        outputs, (hidden, cell) = self.rnn(embedded)  # outputs: [batch, seq_len, hidden]\n        return outputs, (hidden, cell)\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.attn = nn.Linear(hidden_size*2, hidden_size)\n        self.v = nn.Linear(hidden_size, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs, mask=None):\n        # hidden: [1, batch, hidden_size] (last layer from encoder or previous decoder step)\n        # encoder_outputs: [batch, seq_len, hidden_size]\n        batch_size = encoder_outputs.size(0)\n        seq_len = encoder_outputs.size(1)\n        \n        hidden_exp = hidden.permute(1,0,2).repeat(1, seq_len, 1)  # [batch, seq_len, hidden_size]\n        energy = torch.tanh(self.attn(torch.cat((hidden_exp, encoder_outputs), dim=2)))  # [batch, seq_len, hidden]\n        attention = self.v(energy).squeeze(2)  # [batch, seq_len]\n\n        if mask is not None:\n            attention = attention.masked_fill(mask == 0, float('-inf'))\n\n        return torch.softmax(attention, dim=1)  # [batch, seq_len]\n\n\nclass DecoderAttention(nn.Module):\n    def __init__(self, output_size, embed_size, hidden_size):\n        super().__init__()\n        self.embedding = nn.Embedding(output_size, embed_size)\n        self.rnn = nn.LSTM(embed_size + hidden_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size*2, output_size)\n        self.attention = Attention(hidden_size)\n\n    def forward(self, x, hidden_cell, encoder_outputs, mask=None):\n        # x: [batch] (current token indices)\n        # hidden_cell: (h, c) of decoder LSTM\n        x = x.unsqueeze(1)  # [batch, 1]\n        embedded = self.embedding(x)  # [batch,1,embed]\n\n        # Use last layer hidden for attention\n        last_hidden = hidden_cell[0]  # [1,batch,hidden]\n        attn_weights = self.attention(last_hidden, encoder_outputs, mask=mask)  # [batch, seq_len]\n        attn_weights = attn_weights.unsqueeze(1)  # [batch,1,seq_len]\n\n        context = torch.bmm(attn_weights, encoder_outputs)  # [batch,1,hidden]\n        rnn_input = torch.cat((embedded, context), dim=2)  # [batch,1, embed+hidden]\n\n        output, hidden_cell = self.rnn(rnn_input, hidden_cell)  # output: [batch,1,hidden]\n        output = torch.cat((output.squeeze(1), context.squeeze(1)), dim=1)  # [batch, hidden*2]\n        prediction = self.fc(output)  # [batch, output_size]\n\n        return prediction, hidden_cell\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:19:00.738931Z","iopub.execute_input":"2025-12-27T07:19:00.739473Z","iopub.status.idle":"2025-12-27T07:19:00.749076Z","shell.execute_reply.started":"2025-12-27T07:19:00.739448Z","shell.execute_reply":"2025-12-27T07:19:00.748376Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"# Load pretrained embeddings and LSTM weights\npretrained_state = torch.load(\"/kaggle/input/lstm-v2/pytorch/default/1/best_lstm.pt\")\n\nencoder = Encoder(input_size=vocab_size, embed_size=128, hidden_size=256, num_layers=2)\ndecoder = DecoderAttention(output_size=vocab_size, embed_size=128, hidden_size=256)\n\nencoder.to(device)\ndecoder.to(device)\n\n# Copy embeddings\npretrained_vocab_size = 91\nencoder.embedding.weight.data[:pretrained_vocab_size] = pretrained_state['embedding.weight']\ndecoder.embedding.weight.data[:pretrained_vocab_size] = pretrained_state['embedding.weight']\n\n# Initialize new embeddings for special tokens\nnn.init.uniform_(encoder.embedding.weight.data[pretrained_vocab_size:], -0.01, 0.01)\nnn.init.uniform_(decoder.embedding.weight.data[pretrained_vocab_size:], -0.01, 0.01)\n\n# Copy LSTM weights\nencoder.rnn.weight_ih_l0.data.copy_(pretrained_state['lstm.weight_ih_l0'])\nencoder.rnn.weight_hh_l0.data.copy_(pretrained_state['lstm.weight_hh_l0'])\nencoder.rnn.bias_ih_l0.data.copy_(pretrained_state['lstm.bias_ih_l0'])\nencoder.rnn.bias_hh_l0.data.copy_(pretrained_state['lstm.bias_hh_l0'])\n\nencoder.rnn.weight_ih_l1.data.copy_(pretrained_state['lstm.weight_ih_l1'])\nencoder.rnn.weight_hh_l1.data.copy_(pretrained_state['lstm.weight_hh_l1'])\nencoder.rnn.bias_ih_l1.data.copy_(pretrained_state['lstm.bias_ih_l1'])\nencoder.rnn.bias_hh_l1.data.copy_(pretrained_state['lstm.bias_hh_l1'])\n\ndecoder.fc = nn.Linear(256 + 256, vocab_size)  \nnn.init.xavier_uniform_(decoder.fc.weight)\nnn.init.uniform_(decoder.fc.bias, -0.1, 0.1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:34:32.927553Z","iopub.execute_input":"2025-12-27T07:34:32.928170Z","iopub.status.idle":"2025-12-27T07:34:32.968593Z","shell.execute_reply.started":"2025-12-27T07:34:32.928141Z","shell.execute_reply":"2025-12-27T07:34:32.968054Z"}},"outputs":[{"execution_count":109,"output_type":"execute_result","data":{"text/plain":"Parameter containing:\ntensor([-0.0664, -0.0439, -0.0005, -0.0233, -0.0555, -0.0770,  0.0574,  0.0407,\n        -0.0498,  0.0789, -0.0496, -0.0843,  0.0185, -0.0779,  0.0946, -0.0319,\n        -0.0255, -0.0597, -0.0971,  0.0085,  0.0040,  0.0164, -0.0283, -0.0972,\n         0.0810, -0.0186,  0.0799, -0.0591,  0.0170,  0.0179, -0.0751, -0.0936,\n        -0.0329, -0.0220,  0.0110, -0.0909,  0.0458,  0.0608, -0.0966, -0.0569,\n         0.0880,  0.0032,  0.0284,  0.0429,  0.0577, -0.0844,  0.0181, -0.0911,\n        -0.0796, -0.0114,  0.0726, -0.0238,  0.0941, -0.0562,  0.0175, -0.0275,\n        -0.0886, -0.0489,  0.0871, -0.0441,  0.0699,  0.0887, -0.0741,  0.0126,\n        -0.0224,  0.0950, -0.0240,  0.0508, -0.0900, -0.0489, -0.0018,  0.0510,\n         0.0114, -0.0116,  0.0150, -0.0684,  0.0623,  0.0610, -0.0876, -0.0889,\n         0.0627,  0.0571,  0.0407, -0.0575, -0.0723,  0.0418,  0.0406,  0.0454,\n         0.0103, -0.0706, -0.0001,  0.0526,  0.0075,  0.0505, -0.0501],\n       requires_grad=True)"},"metadata":{}}],"execution_count":109},{"cell_type":"code","source":"def create_pad_mask(seq, pad_idx):\n    # seq: [batch, seq_len]\n    return (seq != pad_idx).float()  # 1 = real token, 0 = pad\n\ndef get_batches(inputs, targets, batch_size):\n    for i in range(0, len(inputs), batch_size):\n        yield inputs[i:i+batch_size], targets[i:i+batch_size]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:20:32.445006Z","iopub.execute_input":"2025-12-27T07:20:32.445415Z","iopub.status.idle":"2025-12-27T07:20:32.449785Z","shell.execute_reply.started":"2025-12-27T07:20:32.445383Z","shell.execute_reply":"2025-12-27T07:20:32.449045Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"import torch.optim as optim\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nencoder.to(device)\ndecoder.to(device)\n\n# Hyperparameters\nbatch_size = 16\nnum_epochs = 50\nlr = 0.001\npatience = 5\nmax_grad_norm = 1.0\nmax_seq_len = 60 \n\noptimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\ncriterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n\nbest_val_loss = float('inf')\nepochs_no_improve = 0\n\nfor epoch in range(num_epochs):\n    encoder.train()\n    decoder.train()\n    total_train_loss = 0\n    num_train_batches = 0\n\n    for batch_inputs, batch_targets in train_loader:\n        batch_inputs = batch_inputs[:, :max_seq_len].to(device)\n        batch_targets = batch_targets[:, :max_seq_len].to(device)\n        if batch_targets.size(1) <= 1:\n            continue\n\n        optimizer.zero_grad()\n\n        encoder_outputs, hidden_cell = encoder(batch_inputs)\n        hidden_cell = (hidden_cell[0][-1:].contiguous(), hidden_cell[1][-1:].contiguous())\n\n        input_tok = batch_targets[:, 0]  # <sos>\n        loss = 0\n        seq_len = batch_targets.size(1)\n\n        for t in range(1, seq_len):\n            output, hidden_cell = decoder(input_tok, hidden_cell, encoder_outputs)\n            loss += criterion(output, batch_targets[:, t])\n            input_tok = batch_targets[:, t]  # teacher forcing\n\n        loss = loss / (seq_len - 1)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(list(encoder.parameters()) + list(decoder.parameters()), max_grad_norm)\n        optimizer.step()\n\n        total_train_loss += loss.item()\n        num_train_batches += 1\n        del batch_inputs, batch_targets, encoder_outputs, loss\n        torch.cuda.empty_cache()\n\n    avg_train_loss = total_train_loss / max(1, num_train_batches)\n\n    encoder.eval()\n    decoder.eval()\n    total_val_loss = 0\n    num_val_batches = 0\n\n    with torch.no_grad():\n        for batch_inputs, batch_targets in val_loader:\n            batch_inputs = batch_inputs[:, :max_seq_len].to(device)\n            batch_targets = batch_targets[:, :max_seq_len].to(device)\n            if batch_targets.size(1) <= 1:\n                continue\n\n            encoder_outputs, hidden_cell = encoder(batch_inputs)\n            hidden_cell = (hidden_cell[0][-1:].contiguous(), hidden_cell[1][-1:].contiguous())\n            input_tok = batch_targets[:, 0]\n            seq_len = batch_targets.size(1)\n\n            batch_loss = 0\n            for t in range(1, seq_len):\n                output, hidden_cell = decoder(input_tok, hidden_cell, encoder_outputs)\n                batch_loss += criterion(output, batch_targets[:, t]).item()\n                input_tok = batch_targets[:, t]\n\n            total_val_loss += batch_loss / (seq_len - 1)\n            num_val_batches += 1\n            del batch_inputs, batch_targets, encoder_outputs\n            torch.cuda.empty_cache()\n\n    avg_val_loss = total_val_loss / max(1, num_val_batches)\n\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        epochs_no_improve = 0\n        torch.save({\n            'encoder_state': encoder.state_dict(),\n            'decoder_state': decoder.state_dict(),\n            'vocab': vocab\n        }, 'best_finetuned_model.pt')\n    else:\n        epochs_no_improve += 1\n        if epochs_no_improve >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n    print(f\"Epoch {epoch+1} | Avg Train Loss: {avg_train_loss:.4f} | Avg Val Loss: {avg_val_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:48:34.170730Z","iopub.execute_input":"2025-12-27T07:48:34.171020Z","iopub.status.idle":"2025-12-27T07:49:16.339788Z","shell.execute_reply.started":"2025-12-27T07:48:34.170993Z","shell.execute_reply":"2025-12-27T07:49:16.339046Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 | Avg Train Loss: 1.3219 | Avg Val Loss: 2.0803\nEpoch 2 | Avg Train Loss: 1.1676 | Avg Val Loss: 2.0732\nEpoch 3 | Avg Train Loss: 1.0372 | Avg Val Loss: 2.0468\nEpoch 4 | Avg Train Loss: 0.9523 | Avg Val Loss: 2.0905\nEpoch 5 | Avg Train Loss: 0.8510 | Avg Val Loss: 2.0677\nEpoch 6 | Avg Train Loss: 0.7634 | Avg Val Loss: 2.0850\nEpoch 7 | Avg Train Loss: 0.6971 | Avg Val Loss: 2.0797\nEarly stopping at epoch 8\n","output_type":"stream"}],"execution_count":119},{"cell_type":"code","source":"encoder.eval()\ndecoder.eval()\n\ntotal_correct = 0\ntotal_chars = 0\n\nwith torch.no_grad():\n    for batch_inputs, batch_targets in test_loader:\n        batch_inputs = batch_inputs[:, :max_seq_len].to(device)\n        batch_targets = batch_targets[:, :max_seq_len].to(device)\n        if batch_targets.size(1) <= 1:\n            continue\n\n        encoder_outputs, hidden_cell = encoder(batch_inputs)\n        hidden_cell = (hidden_cell[0][-1:].contiguous(), hidden_cell[1][-1:].contiguous())\n        input_tok = batch_targets[:, 0]\n        seq_len = batch_targets.size(1)\n\n        for t in range(1, seq_len):\n            output, hidden_cell = decoder(input_tok, hidden_cell, encoder_outputs)\n            predicted = output.argmax(dim=1) \n            \n            mask = batch_targets[:, t] != vocab['<pad>']  \n            total_correct += (predicted == batch_targets[:, t]).masked_select(mask).sum().item()\n            total_chars += mask.sum().item()\n\n            input_tok = batch_targets[:, t] \n\nchar_accuracy = total_correct / max(1, total_chars)\nprint(f\"Character-level Accuracy on test set: {char_accuracy*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:57:01.217530Z","iopub.execute_input":"2025-12-27T07:57:01.218085Z","iopub.status.idle":"2025-12-27T07:57:01.520877Z","shell.execute_reply.started":"2025-12-27T07:57:01.218054Z","shell.execute_reply":"2025-12-27T07:57:01.520144Z"}},"outputs":[{"name":"stdout","text":"Character-level Accuracy on test set: 53.85%\n","output_type":"stream"}],"execution_count":133},{"cell_type":"code","source":"import torch\n\ndef translate_normal_to_royal(input_text, encoder, decoder, vocab, max_len=100, device='cuda'):\n    # Prepare input sequence\n    encoder.eval()\n    decoder.eval()\n    \n    inv_vocab = {v:k for k,v in vocab.items()}  # convert indices to characters\n    input_indices = [vocab.get(c, vocab['<unk>']) for c in input_text]\n    input_tensor = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0).to(device)  # [1, seq_len]\n\n    with torch.no_grad():\n        # Encode\n        encoder_outputs, hidden_cell = encoder(input_tensor)\n        hidden_cell = (hidden_cell[0][-1:].contiguous(), hidden_cell[1][-1:].contiguous())\n\n        input_tok = torch.tensor([vocab.get('<sos>', 0)], dtype=torch.long).to(device)\n        generated_indices = []\n\n        for _ in range(max_len):\n            output, hidden_cell = decoder(input_tok, hidden_cell, encoder_outputs)\n            predicted_tok = output.argmax(dim=1)\n            if predicted_tok.item() == vocab.get('<eos>', -1):\n                break\n            generated_indices.append(predicted_tok.item())\n            input_tok = predicted_tok  # feed predicted token as next input\n\n    # Convert indices back to characters\n    generated_text = ''.join([inv_vocab[i] for i in generated_indices])\n    return generated_text\n\nfor i in range(70, 80):\n    sample_text = test_df['input_ids'].iloc[i]  \n    # convert indices to characters\n    sample_text_chars = ''.join([itos[i] for i in sample_text if i not in [vocab['<pad>'], vocab['<sos>'], vocab['<eos>']]])\n    \n    translation = translate_normal_to_royal(sample_text_chars, encoder, decoder, vocab, device=device)\n    print(\"Input text: \", sample_text_chars)\n    print(\"Predicted royal text: \", translation)\n    print(\"=\"*50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T07:56:11.487345Z","iopub.execute_input":"2025-12-27T07:56:11.488012Z","iopub.status.idle":"2025-12-27T07:56:11.757910Z","shell.execute_reply.started":"2025-12-27T07:56:11.487980Z","shell.execute_reply":"2025-12-27T07:56:11.757192Z"}},"outputs":[{"name":"stdout","text":"Input text:  គួរតែគិតធ្វើអ្វីឱ្យមានឈ្មោះល្បីនៅថ្ងៃមុខ\nPredicted royal text:  ព្រះអង្គមានព្រះរាជបុត្រ ឱ្យមានព្រះអង្គ\n==================================================\nInput text:  ស្ដេចតែងតែឱ្យតម្លៃលើស្ត្រីខ្មែរ\nPredicted royal text:  ព្រះអង្គទ្រង់បានតែលព្រះទ័យជំនឹកថ្វាយ\n==================================================\nInput text:  ស្ដេចបានធ្វើការងារនយោបាយក្រៅប្រទេស\nPredicted royal text:  ព្រះអង្គបានយាងទៅក្នុងរាជវិយចាន\n==================================================\nInput text:  ស្ដេចឱ្យដីស្រែដល់អ្នកក្រ\nPredicted royal text:  ព្រះអង្គឱ្យសាងសេចក្ដីលុវត្ដី\n==================================================\nInput text:  ស្ដេចបានធ្វើផ្លូវខ្វាត់ខ្វែងពេញប្រទេស\nPredicted royal text:  ព្រះអង្គប្រាស់ព្រះនាងត្រូវបានប្រាប់ព្រះនាង\n==================================================\nInput text:  តែលោកចាញ់ ហើយក៏ថយទៅកេណ្ឌទ័ពបន្ថែម\nPredicted royal text:  ព្រះអង្គគឺ ព្រះអង្គទ្រង់បានប្រទានរបស់ព្រះអង្គ\n==================================================\nInput text:  ចាប់ពីថ្ងៃនោះមក លោកឈប់ទៅសំពះឪពុក\nPredicted royal text:  ព្រះអង្គទ្រង់ព្រះទ័យទ្រង់តែមហាម និង ព្រះនាង ទ្រង់តាម\n==================================================\nInput text:  ស្ដេចក៏ទៅរស់នៅឯបន្ទាយស្ទឹងសែន\nPredicted royal text:  ព្រះអង្គមានព្រះរាជទ្រង់មានព្រះបាទសម្លាប់ទៅព្រះនាង\n==================================================\nInput text:  លោកនឹងកើតជាកូនរបស់ស្ដេច\nPredicted royal text:  បានតាមរាជទេវតា បានសង្គាប់ព្រះរាជទាំ\n==================================================\nInput text:  ស្ដេចឱ្យមាសប្រាក់ដល់អ្នកស្មោះត្រង់\nPredicted royal text:  មានព្រះរាជហឫទ័យយាងទៅត្រូវត្រមាន់ប្រាស់\n==================================================\n","output_type":"stream"}],"execution_count":132}]}