---
title: "LSTM-based Khmer Text Style Transfer Using Representation Learning"
author: 
  - "Pel Bunkhloem Phoeurn Kimhor"
  - "Pheth Soriyuon Phorn Sreypov"
  - "Phoeun Rajame Yin Sambat"
  - "**Supervised by:** Dr. KHON Vanny"
date: "02 January 2026"
format: 
  revealjs:
    slide-number: true
    transition: slide
    scrollable: true
    theme: white
    progress: true
    smaller: true
    embed-resources: true
    number-sections: true

include-in-header:
  - text: |
      <style>
      .title {
        font-size: 1.5em !important; /* Adjust this number to make it smaller */
      }
      .reveal .slide-background.present::before {
        content: '';
        position: absolute;
        top: 10px;
        left: 10px;
        width: 120px;
        height: 120px;
        background-image: url('../assets/itc.png');
        background-size: contain;
        background-repeat: no-repeat;
        z-index: 10;
      }
      .reveal .slide-background.present::after {
        content: '';
        position: absolute;
        top: 10px;
        right: 10px;
        width: 200px;
        height: 200px;
        background-image: url('../assets/ams_t.png');
        background-size: contain;
        background-repeat: no-repeat;
        z-index: 10;
      }
      .reveal .slides > section.present {
        position: relative;
      }
      .title-logos {
        position: absolute;
        top: 10px;
        width: 100%;
        display: flex;
        justify-content: space-between;
        padding: 0 20px;
        box-sizing: border-box;
        z-index: 20;
        pointer-events: none;
      }
      .title-logos img {
        height: 100px;
        width: auto;
      }
      .reveal h1 {
        font-size: 1.5em !important;
      }
      .reveal h2,
      .reveal h3 {
        font-size: 0.7em;
      }
      </style>
---

# Problem Statement

The task is formulated as a **sequence-to-sequence style transfer problem**, where an input sentence in normal Khmer is transformed into its corresponding royal-style sentence.

Let:

* $X = (x_1, x_2, \dots, x_T)$ denote the input sequence
* $Y = (y_1, y_2, \dots, y_{T'})$ denote the output sequence
* $f_\theta$ denote an encoder–decoder LSTM model

---

## Model Objective and Notation

<div style="display: flex; gap: 2rem; align-items: flex-start; font-size: 0.85em;">

<div style="flex: 0.65;">

<strong style="color: #1f77b4;">Conditional Probability</strong>


$$
P(Y \mid X; \theta)
=
\prod_{t=1}^{T'}
P\!\left(y_t \mid y_1, \dots, y_{t-1}, X; \theta\right)
$$

<strong style="color: #1f77b4;">Training Objective</strong>

$$
\theta^*
=
\arg\max_{\theta}
\sum_{(X,Y)\in\mathcal{D}}
\log P(Y \mid X; \theta)
$$

</div>

<div style="flex: 0.35; font-size: 0.9em;">

<strong style="color: #1f77b4;">Notation</strong>

- $x_t, y_t$ : tokens at time step $t$  
- $T, T'$ : input and output lengths  
- $\theta$ : model parameters  
- $h_t$ : encoder hidden state  

</div>

</div>

---

#### Attention Mechanism

The mechanism begins by calculating how well each encoder state $h_s^{enc}$ matches the current decoder needs.

**Alignment Score:** Measures the relevance of input $s$ at decoding step $t$:
$$e_{t,s} = h_{t-1}^{dec} \cdot h_s^{enc}$$

**Attention Weight:** Normalizes scores into a probability distribution using Softmax:
$$\alpha_{t,s} = \frac{\exp(e_{t,s})}{\sum_{k=1}^{T} \exp(e_{t,k})}$$

The model then extracts relevant information to generate the final character.

**Context Vector ($c_t$):** A weighted sum of all encoder hidden states:
$$c_t = \sum_{s=1}^{T} \alpha_{t,s} h_s^{enc}$$

**Final Prediction:** The decoder hidden state $h_t^{dec}$ is updated with $c_t$, and the next character is predicted:
$$P(y_t \mid y_{<t}, X) = \text{Softmax}(W_{hy} h_t^{dec} + b_y)$$

---

## Experimental Summary

- Two char-level Seq2Seq models pre-trained on different corpora:
  - **General text** (news, public modern Khmer)
  - **Folktale text** (classical Khmer folktales)
- Shared pre-training hyperparameters: Hidden 256, Layers 2, Batch 32, LR 0.001, Optimizer Adam, Epochs 50, Loss Cross-entropy.
- Outcome: General-text model showed higher predictive accuracy and better coverage of modern Khmer syntax/vocabulary.

---

## Fine-tuning Setup

- Base model: **General-text** pre-trained Seq2Seq.
- Hyperparameters: LR 0.001, Batch 16, Epochs 100, Optimizer Adam, Loss Cross-entropy.
- Special mechanism: **Attention** to improve alignment and decoding.

---

## Results

**Pre-training BLEU (test):**

| Model        | BLEU (%) |
|--------------|----------|
| General Text | **30.1** |
| Folktale Text| **9.4**  |

**Fine-tuning (samples):** BLEU scores range **0.73–0.87** across representative sentences, with outputs reflecting royal style and coherent structure.

---

## Discussion

- **Corpus choice matters:** General-text pre-training yields richer representations that transfer better.
- **Folktale bias:** Archaic/repetitive patterns reduce generalization and BLEU.
- **Attention helps:** Improves alignment and word order, especially for longer Khmer sentences.